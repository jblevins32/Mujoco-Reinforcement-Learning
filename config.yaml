############### REGULAR TRAINING/TESTING ###############
# Training specific parameters
num_environments: 64
epochs: 250 # 4000 - normal, 250 - adversarial
t_steps: 500 # Number of rollouts per epoch
save_every: 4 # How many epochs to save the model (or adv_iters for adversarial training)
discount: 0.99
lr: .002
rl_alg_name: "PPO_CONT"
gym_model_train: "Ant-v5" 
space: "cont" # disc or cont depending on env
load_dict: False # Load a model from a dictionary
load_path: "2.11771" # Load model path

# Testing specific parameters
gym_model_test: "MRPP_Env" 
test_model_reward: "2.38366_2025-03-09_07-11" # Final reward of saved model (to differentiate them)
test_steps: 500
record_period: 1

### Current env options:
# Humanoid-v5 
# Pusher-v5
# HalfCheetah-v5 
# MountainCar-v0 
# CartPole-v1 
# Ant-v5 
# Swimmer-v5
# MRPP_Env

### Current working rl alg options:
# SAC
# PPO

############### Adversarial parameters ###############
adv_iter: 20

############### MY MRPP SIMULATION ###############
# General
num_agents: 10
agent_radius: .5
map_size: [100,50]
num_obstacles: 5
obstacle_radius_max: 15
dynamics: "single_integrator"  # "unicycle" or "double_integrator"
dt: 0.01
w_dist: 1
w_coll: 5
w_dir: 5
w_goal: 1
done_threshold: 5
u_clip: 100 # bound the control command to mitigate jerky behavior
seed_value: 42

# Decentralized case
communication_range: 10.0
observation_range: 10.0
centralized: False

# Learning case
obstacle_cost: 5.0
max_episode_steps: 350

# Attack case
test_attack: True
display_attack: True